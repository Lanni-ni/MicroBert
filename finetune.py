# -*- coding: utf-8 -*-
"""microbert.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vfknxm77SbPD3-3xLch3s9xPCMc5P0u_
"""

from google.colab import drive
drive.mount('/content/drive')

!pip install transformers

!pip install spacy-transformers

!pip uninstall -y numpy
!pip install numpy==1.23.5

!pip install --upgrade cython
!pip install --force-reinstall --no-cache-dir thinc

!sudo apt-get update -y
!sudo apt-get install python3.10 python3.10-dev python3.10-venv -y


!python3.10 -m venv /usr/local/envs/py310


!/usr/local/envs/py310/bin/python -m pip install --upgrade pip
!/usr/local/envs/py310/bin/pip install torch==1.12.1+cu116 torchvision==0.13.1+cu116 -f https://download.pytorch.org/whl/torch_stable.html

!/usr/local/envs/py310/bin/pip install numpy==1.23.5 "spacy<3.6.0"
!/usr/local/envs/py310/bin/pip install git+https://github.com/allenai/allennlp.git@v2.10.1
!/usr/local/envs/py310/bin/pip install git+https://github.com/allenai/allennlp-models.git@v2.10.1

!/usr/local/envs/py310/bin/python -m allennlp test-install

# Load model directly
from transformers import AutoTokenizer, AutoModel

tokenizer = AutoTokenizer.from_pretrained("lgessler/microbert-coptic-mxp")
model = AutoModel.from_pretrained("lgessler/microbert-coptic-mxp")

import pandas as pd
from transformers import AutoTokenizer
from torch.utils.data import Dataset, DataLoader
import torch

train_df = pd.read_csv("/content/drive/MyDrive/final_train_BIO.csv")
dev_df = pd.read_csv("/content/drive/MyDrive/final_dev_BIO.csv")

from sklearn.preprocessing import LabelEncoder
import torch
from torch.utils.data import Dataset, DataLoader
from transformers import AutoTokenizer
import pandas as pd

import re

def normalize_label(label):
    if pd.isna(label):
        return label
    match = re.match(r"^[BI]-(\w+)", label)
    if match:
        return label[:2] + "-" + match.group(1)
    else:
        return label


train_df["Label"] = train_df["Label"].apply(normalize_label)
dev_df["Label"] = dev_df["Label"].apply(normalize_label)

train_df["is_sep"] = train_df["Token"].isna() | (train_df["Token"].astype(str).str.strip().isin(["", "nan", "NaN"]))
dev_df["is_sep"] = dev_df["Token"].isna() | (dev_df["Token"].astype(str).str.strip().isin(["", "nan", "NaN"]))

#
print(" lines（is_sep=True）:", train_df["is_sep"].sum())

train_df['Token'] = train_df['Token'].astype(str)
dev_df['Token'] = dev_df['Token'].astype(str)


tokenizer = AutoTokenizer.from_pretrained("lgessler/microbert-coptic-mxp")


label_encoder = LabelEncoder()


all_labels = train_df['Label'].tolist() + dev_df['Label'].tolist()
label_encoder.fit(all_labels)


print(f"labels：{dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)))}")

print(sorted(set(all_labels)))

def group_sentences(df):
    sentences = []
    current_tokens = []
    current_labels = []

    for _, row in df.iterrows():
        token = row["Token"]
        label = row["Label"]
        if row["is_sep"]:
            if current_tokens:
                sentences.append((current_tokens, current_labels))
                current_tokens = []
                current_labels = []
        else:
            current_tokens.append(token)
            current_labels.append(label)

    if current_tokens:
        sentences.append((current_tokens, current_labels))

    return sentences

train_sentences = group_sentences(train_df)
dev_sentences = group_sentences(dev_df)

import pickle

with open("/content/drive/MyDrive/train_sentences.pkl", "wb") as f:
    pickle.dump(train_sentences, f)

with open("/content/drive/MyDrive/dev_sentences.pkl", "wb") as f:
    pickle.dump(dev_sentences, f)

train_sentences = group_sentences(train_df)
print("sentences：", len(train_sentences))
print("examples：", train_sentences[0])

print(train_sentences[0][0])

print(train_sentences[0][1])

dataset_code = '''
from torch.utils.data import Dataset, DataLoader
from torch.nn.utils.rnn import pad_sequence
import torch

class NERDataset(Dataset):
    def __init__(self, sentence_list, tokenizer, label_encoder, max_length=128):
        self.sentences = sentence_list
        self.tokenizer = tokenizer
        self.label_encoder = label_encoder
        self.max_length = max_length

    def __len__(self):
        return len(self.sentences)

    def __getitem__(self, idx):
        try:
            tokens, labels = self.sentences[idx]
            encoding = self.tokenizer(
                tokens,
                is_split_into_words=True,
                truncation=True,
                padding='max_length',
                max_length=self.max_length,
                return_tensors='pt'
            )
            word_ids = encoding.word_ids(batch_index=0)
            label_ids = []
            for word_id in word_ids:
                if word_id is None:
                    label_ids.append(-100)
                else:
                    label_ids.append(self.label_encoder.transform([labels[word_id]])[0])

            return {
                'input_ids': encoding['input_ids'].squeeze(0),
                'attention_mask': encoding['attention_mask'].squeeze(0),
                'labels': torch.tensor(label_ids, dtype=torch.long).view(-1)
            }

        except Exception as e:
            print(f" Error at idx={idx}: {e}")
            raise

def collate_fn(batch):
    input_ids = [item['input_ids'] for item in batch]
    attention_mask = [item['attention_mask'] for item in batch]
    labels = [item['labels'] for item in batch]

    return {
        'input_ids': pad_sequence(input_ids, batch_first=True, padding_value=0),
        'attention_mask': pad_sequence(attention_mask, batch_first=True, padding_value=0),
        'labels': pad_sequence(labels, batch_first=True, padding_value=-100)
    }
'''
with open('/content/drive/MyDrive/ner_dataset.py', 'w') as f:
    f.write(dataset_code)

import sys
sys.path.append('/content/drive/MyDrive')

from ner_dataset import NERDataset, collate_fn
train_dataset = NERDataset(train_sentences, tokenizer, label_encoder)

print(f"🔍 Dataset size: {len(train_dataset)}")

train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True, collate_fn=collate_fn)

for batch in train_dataloader:
    print("input_ids shape:", batch["input_ids"].shape)
    print("labels shape:   ", batch["labels"].shape)
    break

model_code = '''
import torch
import torch.nn as nn
import torch.nn.functional as F
from transformers import BertModel
from allennlp.modules import ConditionalRandomField

class BiLSTM_CRF_Model(nn.Module):
    def __init__(self, bert_model_name, hidden_size=200, num_labels=22, dropout=0.5):
        super(BiLSTM_CRF_Model, self).__init__()


        self.bert = BertModel.from_pretrained(bert_model_name)


        self.lstm = nn.LSTM(input_size=self.bert.config.hidden_size,
                            hidden_size=hidden_size,
                            num_layers=2,
                            bidirectional=True,
                            dropout=dropout,
                            batch_first=True)


        self.fc = nn.Linear(hidden_size * 2, num_labels)


        self.crf = ConditionalRandomField(num_labels)


        self.dropout = nn.Dropout(dropout)

    def forward(self, input_ids, attention_mask, labels=None):
        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)
        sequence_output = outputs.last_hidden_state

        lstm_output, _ = self.lstm(sequence_output)
        lstm_output = self.dropout(lstm_output)
        logits = self.fc(lstm_output)  # shape: [batch_size, seq_len, num_labels]

        mask = attention_mask.bool()

        if labels is not None:
            labels = labels.long()

            labels_for_crf = labels.clone()
            labels_for_crf[labels_for_crf == -100] = 0

            loss = -self.crf(logits, labels_for_crf, mask=mask)
            return loss
        else:
            best_paths = self.crf.viterbi_tags(logits, mask)
            preds = [tags for tags, score in best_paths]
            return preds
'''
with open("bilstm_crf_model.py", "w") as f:
    f.write(model_code)

from google.colab import drive
drive.mount('/content/drive')

!cp bilstm_crf_model.py /content/drive/MyDrive/

train_code = '''
import torch
from torch.utils.data import DataLoader
from torch.optim import AdamW
from sklearn.metrics import f1_score
import pickle

from transformers import AutoTokenizer
from sklearn.preprocessing import LabelEncoder

from ner_dataset import NERDataset, collate_fn
from bilstm_crf_model import BiLSTM_CRF_Model


with open("/content/drive/MyDrive/train_sentences.pkl", "rb") as f:
    train_sentences = pickle.load(f)
with open("/content/drive/MyDrive/dev_sentences.pkl", "rb") as f:
    dev_sentences = pickle.load(f)
tokenizer = AutoTokenizer.from_pretrained("lgessler/microbert-coptic-mxp")
label_list = sorted(list({label for sent in train_sentences + dev_sentences for label in sent[1]}))
label_encoder = LabelEncoder()
label_encoder.fit(label_list)


train_dataset = NERDataset(train_sentences, tokenizer, label_encoder)
dev_dataset = NERDataset(dev_sentences, tokenizer, label_encoder)
train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True, collate_fn=collate_fn)
dev_dataloader = DataLoader(dev_dataset, batch_size=16, shuffle=False, collate_fn=collate_fn)


device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = BiLSTM_CRF_Model(bert_model_name="lgessler/microbert-coptic-mxp", num_labels=len(label_list)).to(device)


start_epoch = 110
model.load_state_dict(torch.load(f"/content/drive/MyDrive/checkpoint_epoch{start_epoch}.pth"))
print(f" checkpoint_epoch{start_epoch}.pth")

optimizer = AdamW(model.parameters(), lr=2e-5)

epochs = 300
patience = 50
best_f1 = 0
counter = 0

for epoch in range(start_epoch, epochs):
    model.train()
    total_loss = 0
    for batch in train_dataloader:
        input_ids = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)
        labels = batch['labels'].to(device)

        optimizer.zero_grad()
        loss = model(input_ids, attention_mask, labels)
        total_loss += loss.item()
        loss.backward()
        optimizer.step()

    avg_train_loss = total_loss / len(train_dataloader)
    print(f"Epoch {epoch + 1}/{epochs}, Loss: {avg_train_loss:.4f}")


    if (epoch + 1) % 10 == 0:
        model.eval()
        all_preds = []
        all_labels = []
        with torch.no_grad():
            for batch in dev_dataloader:
                input_ids = batch['input_ids'].to(device)
                attention_mask = batch['attention_mask'].to(device)
                labels = batch['labels'].to(device)

                preds = model(input_ids, attention_mask)
                for pred_seq, gold_seq, mask in zip(preds, labels, attention_mask):
                    for p, g, m in zip(pred_seq, gold_seq, mask):
                        if m == 1 and g != -100:
                            all_preds.append(p)
                            all_labels.append(g.item())

        f1 = f1_score(all_labels, all_preds, average='weighted', labels=list(range(len(label_list))))
        print(f" Dev F1 @ Epoch {epoch + 1}: {f1:.4f}")

        if f1 > best_f1:
            best_f1 = f1
            torch.save(model.state_dict(), "/content/drive/MyDrive/best_model.pth")
            print(f"  best_model @ epoch {epoch + 1}")
            counter = 0
        else:
            counter += 1

        ckpt_path = f"/content/drive/MyDrive/checkpoint_epoch{epoch + 1}.pth"
        torch.save(model.state_dict(), ckpt_path)
        print(f" checkpoint: {ckpt_path}")

        if counter >= patience:
            print(f"Early stopping at epoch {epoch + 1}")
            break


'''


with open("train_bilstm_crf.py", "w") as f:
    f.write(train_code)

!cp train_bilstm_crf.py /content/drive/MyDrive/

!source /usr/local/envs/py310/bin/activate && \
/usr/local/envs/py310/bin/python /content/drive/MyDrive/train_bilstm_crf.py
